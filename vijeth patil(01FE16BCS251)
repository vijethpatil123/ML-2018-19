part 1:
3) learning rate=0.0005 and iterations=100
   cost= 1275.3828 W= 7.084465 b= 2.8597229

   learning rate=0.0005 and iterations=500
   cost= 622.44434 W= 15.310479 b= 8.409555 

   learning rate=0.0005 and iterations=1000
   cost= 560.63306 W= 15.413998 b= 12.24086 
   
   learning rate=0.005 and iterations=100
   cost= 560.4982 W= 15.3212805 b= 12.233944 

   learning rate=0.005 and iterations=500
   cost= 273.51242 W= 8.818607 b= 35.145744 

   learning rate=0.005 and iterations=1000
   cost= 116.7167 W= 3.3937647 b= 54.004894 

4) Unfortunately, there is no right answer to this question. The answer is different for different datasets but you can say that the numbers
    epochs is related to how diverse your data is.


Part 2:
1) As no of epochs increases the cost function decreases because if there is only one epoch the curve will be underfitted and if we just
increase the no of epochs the curve will be optimum, if epochs are large the curve becomes overfitted. 

2) Time complexity: when the no of epochs is large, it is quite obvious that it takes more time to update weights to the neural network.
 
3) for teta1=0 and teta2=0 ,learning rate =0.005 and epochs=500 the cost was reduced to 1178 to 622 and the final values for teta1 and teta2 
   were	15 and 8. so we cant choose random values and say that cost function obtained is minimum. sometimes for large no of iterartions where 
   cost may be overshooted.

part 3:

1)  we use iterative algorithm to find minimum cost.
    gradient descent is an iterative optimization algorithm to find the best results (minima of a curve) that is we find rate of 
    inclination or declination of a slope. finally the optimal result is given and helps a under-fitted graph to make the graph fit 
    optimally to the data.




